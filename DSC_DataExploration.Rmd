---
title: "Data Science Capstone - Data Exploration"
author: "Michael Coote"
date: "2/24/2019"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Objective

Data Science Capston Project

## Get the dataset

```{r, cache=TRUE}
url_f <- 
  "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
file_data <- "Coursera-SwiftKey.zip"
# download.file(url = url_f, destfile = file_data)
file <- list.files(pattern = "zip", full.names = TRUE)
file.size(file) / 1e6
unzip(file)
folder <- "final"
files_data <- list.files(folder, full.names = TRUE, recursive = TRUE)
files_data_sh <- list.files(folder, full.names = FALSE, recursive = TRUE)
files_data_sh <- gsub("^(.*)/", "", files_data_sh)
files_data_info <- file.info(files_data, recursive = TRUE)
files_data_info$size <- files_data_info$size / 1e6
files_data_info <- rename(files_data_info, size_Mb = size)
files_data_info
```

Total size of data = `r sum(files_data_info$size_Mb)` in 
`r nrow(files_data_info)` files.

## File Exploration

Show Filenames and first line of each file

```{r, cache=TRUE}
files_data
data <- lapply(files_data, read_lines, n_max = 1)
names(data) <- files_data_sh
data
```

Find US files only 

```{r}
files_data_US <- files_data[grepl("US", files_data)]
files_data_sh_US <- files_data_sh[grepl("US", files_data_sh)]
```

Load all US into memory

```{r}
data <- lapply(files_data_US, read_lines)
names(data) <- files_data_sh_US
names(data)
```

File space in memory = `r format(object.size(data), units = "Mb")`

Show number of lines in each file

```{r}
prettyNum(lapply(data, length), big.mark = ",")
```

## Exploratory Analysis

### Word Distribution and Relationships

1. Some words are more frequent than others - what are the distributions of word
frequencies?
1. What are the frequencies of 2-grams and 3-grams in the dataset?
1. How many unique words do you need in a frequency sorted dictionary to cover 
50% of all word instances in the language? 90%?
1. How do you evaluate how many of the words come from foreign languages?
1. Can you think of a way to increase the coverage -- identifying words that may 
not be in the corpora or using a smaller number of words in the dictionary to 
cover the same number of phrases?

```{r}

```


--------------------------------------------------------

---
title: "Data Science Capstone - Data Exploration"
author: "Michael Coote"
date: "2/24/2019"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tm)
library(wordcloud2)
library(pander)
library(ggplot2)
```

## Objective

Data Science Capston Project

## Get the dataset

```{r, cache=TRUE}
url_f <- 
  "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
file_data <- "Coursera-SwiftKey.zip"
# download.file(url = url_f, destfile = file_data)
file <- list.files(pattern = "zip", full.names = TRUE)
file.size(file) / 1e6
unzip(file)
folder <- "final"
files_data <- list.files(folder, full.names = TRUE, recursive = TRUE)
files_data_sh <- list.files(folder, full.names = FALSE, recursive = TRUE)
files_data_sh <- gsub("^(.*)/", "", files_data_sh)
files_data_info <- file.info(files_data, recursive = TRUE)
files_data_info$size <- round(files_data_info$size / 1e6, 1)
files_data_info <- rename(files_data_info, size_Mb = size)
files_data_info
```

Total size of data = `r sum(files_data_info$size_Mb)` Mb in 
`r nrow(files_data_info)` files.

## File Exploration

Show Filenames and first line of each file

```{r, cache=TRUE}
data <- sapply(files_data, read_lines, n_max = 1)
data
```

Find the US files only load into memory.

```{r}
files_data_US <- files_data[grepl("US", files_data)]
files_data_sh_US <- files_data_sh[grepl("US", files_data_sh)]
data <- lapply(files_data_US, read_lines)
names(data) <- files_data_sh_US
```

The files `r pander(names(data))` are loaded. They are using 
`r format(object.size(data), units = "Mb")` of memory.

Each file has the following number of rows:

```{r}
enframe(prettyNum(lapply(data, length), big.mark = ","), 
        name = "file", value = "rows") 
```

### Data Cleaning

The US files are 

## Exploratory Analysis

### Word Distribution and Relationships

Some words are more frequent than others - what are the distributions of word
frequencies?

```{r}

```

What are the frequencies of 2-grams and 3-grams in the dataset?

```{r}

```

How many unique words do you need in a frequency sorted dictionary to cover 
50% of all word instances in the language? 90%?

```{r}

```

How do you evaluate how many of the words come from foreign languages?


1. Can you think of a way to increase the coverage -- identifying words that may 
not be in the corpora or using a smaller number of words in the dictionary to 
cover the same number of phrases?

```{r}

```


--------------------------------------------------------
